"""
PCA 降维。
由于图像具有很高的维数，在许多计算机视觉应用中，我们经常使用降维操作。
PCA 产生的投影矩阵可以被视为将原始坐标变换到现有的坐标系，坐标系中的各个坐标按照重要性递减排列。
通过将数据矩阵与 投影矩阵相乘，并可得到降维后的数据矩阵（也即将原始数据从原来的数据维度投影到现在的新的数据维度）
"""

# 为了对图像数据进行 PCA 变换，图像需要转换成一维向量表示。我们可以使用 NumPy 类库中的 flatten() 方法进行变换。
# 将变平的图像堆积起来，我们可以得到一个矩阵，矩阵的一行表示一幅图像。
# 在计算主方向之前，所有的行图像按照平均图像进行了中心化。我们通常使用 SVD（Singular Value Decomposition，奇异值分解）方法来计算主成分；
# 但当矩阵的维数很大时，SVD 的计算非常慢，所以此时通常不使用 SVD 分解。

import numpy as np


def pca(X):
    """ 主成分分析：
    输入：矩阵 X，其中该矩阵中存储训练数据，每一行为一条训练数据
    返回：投影矩阵（按照维度的重要性排序）、方差和均值 """
    # 获取维数
    num_data, dim = X.shape
    # 数据中心化
    mean_X = X.mean(axis=0)
    X = X - mean_X

    # 如果数据个数小于向量的维数，我们不用 SVD 分解，而是计算维数更小的协方差矩阵 XX^T 的特征向量。
    # 通过仅计算对应前 k（k 是降维后的维数）最大特征值的特征向量，可以使上面的 PCA 操作更快。
    if dim > num_data:
        # PCA- 使用紧致技巧
        M = np.dot(X, X.T)  # 协方差矩阵
        e, EV = np.linalg.eigh(M)  # 特征值和特征向量
        tmp = np.dot(X.T, EV).T  # 这就是紧致技巧
        V = tmp[::-1]  # 由于最后的特征向量是我们所需要的，所以需要将其逆转
        S = np.sqrt(e)[::-1]  # 由于特征值是按照递增顺序排列的，所以需要将其逆转
        for i in range(V.shape[1]):
            V[:, i] /= S
    else:
        # PCA- 使用 SVD 方法
        U, S, V = np.linalg.svd(X)
        V = V[:num_data]    # 仅仅返回前 nun_data 维的数据才合理
    # 返回投影矩阵、方差和均值
    return V, S, mean_X
    # V = mxn : m 表示数据条数（即数据矩阵的行数）；n 表示单个数据的长度（即数据维数，等于数据矩阵的列数）；


if __name__ == '__main__':
    pass
